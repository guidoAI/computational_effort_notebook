{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational effort\n",
    "In [1], Viola and Jones introduced a very efficient object detection algorithm. As most other object detectors of the time, it sees object detection as a binary _classification problem_; It runs a window over the entire image at various scales and for each sample classifies it as the object of interest or not. The computational efficiency of the method derives from the use of integral images - allowing the quick determination of 'Haar' feature values irrespective of their scale - and the use of cascades of weak classifiers - allowing to quickly reject most samples, before extracting too many features from the sample.\n",
    "\n",
    "Below, you  will play with the parameters of the algorithm, as executed by OpenCV, and gauge the impact on the processing time.\n",
    "\n",
    "<font color='red'><B>Exercise 1.</B></font> First check if you can use your webcam. This can only be the case if you run the notebook locally. You can test it by running the code in the cell below. Depending on whether it works, go the right part of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam is working.\n"
     ]
    }
   ],
   "source": [
    "import live\n",
    "live.check_webcam();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam works\n",
    "<font color='red'><B>Exercise 2.</B></font>\n",
    "The below code imports the file `live.py`, which detects faces in the images coming from your webcam <A HREF=\"https://github.com/guidoAI/computational_effort_notebook/blob/master/live.py\" TARGET=\"_blank\">(link to file)</A>. It runs the OpenCV face detector on the images. You will see a new window that shows the images and detections (with green boxes), and you will see below a print-out with a carriage return of the number of faces found and the time that the algorithm takes in seconds.\n",
    "\n",
    "1. Can you make the execution time larger or smaller by manipulating the content of the images? How is the execution time related to the content? \n",
    "\n",
    "You can stop the script by clicking the 'Stop' symbol on the toolbar (next to 'Run')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 faces, in 0.047875165939331055 seconds..\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7d7026fc210f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\computational_effort_notebook\\live.py\u001b[0m in \u001b[0;36mrun_detection\u001b[1;34m(scale_factor, min_neighbors, min_size)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_neighbors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[1;31m#flags = cv2.CV_HAAR_SCALE_IMAGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \t); \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "live.run_detection();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><B>Exercise 3.</B></font>\n",
    "Below, you can see the code that was run above. The function `run_detection` takes three parameters:\n",
    "`scale_factor`, `min_neighbors`, and `min_size` (<A HREF=\"https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html\" TARGET=\"_blank\">documentation of the OpenCV function</A>).\n",
    "\n",
    "1. Can you make the algorithm run much faster by changing the parameters? Why does it become much faster? Does it influence the performance of the algorithm in any significant way?\n",
    "2. Can you make the algorithm much faster by other means, e.g., adding more parameters or intervening (adding / changing code) in the `run_detection` function? How, and why does the execution time change? Does this affect the performance in any significant way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 faces, in 0.04487729072570801 seconds..\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e28092fd105d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mmin_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mrun_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e28092fd105d>\u001b[0m in \u001b[0;36mrun_detection\u001b[1;34m(scale_factor, min_neighbors, min_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_neighbors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[1;31m#flags = cv2.CV_HAAR_SCALE_IMAGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \t); end_time = time.time();\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "def run_detection(scale_factor = 1.1, min_neighbors = 5, min_size = 30):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if(not cap.isOpened()):  \n",
    "        print('Video capture not initialized!');\n",
    "        return -1;\n",
    "    \n",
    "    # Create the haar cascade\n",
    "    faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    while(True):\n",
    "    \t# Capture frame-by-frame\n",
    "    \tret, frame = cap.read(); start_time = time.time();\n",
    "        \n",
    "    \t# Our operations on the frame come here\n",
    "    \tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \t# Detect faces in the image\n",
    "    \tfaces = faceCascade.detectMultiScale(\n",
    "    \t\tgray,\n",
    "    \t\tscaleFactor=scale_factor,\n",
    "    \t\tminNeighbors=min_neighbors,\n",
    "    \t\tminSize=(min_size, min_size),\n",
    "    \t\t#flags = cv2.CV_HAAR_SCALE_IMAGE\n",
    "    \t); end_time = time.time();\n",
    "    \n",
    "    \tprint(\"Found {0} faces, in {1} seconds.\".format(len(faces), end_time - start_time), end='\\r')\n",
    "    \n",
    "    \t# Draw a rectangle around the faces\n",
    "    \tfor (x, y, w, h) in faces:\n",
    "    \t\tcv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    \n",
    "    \t# Display the resulting frame\n",
    "    \tcv2.imshow('frame', frame)\n",
    "    \tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    \t\tbreak\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# play with the parameters:\n",
    "scale_factor = 1.1;\n",
    "min_neighbors = 5;\n",
    "min_size = 30;\n",
    "run_detection(scale_factor, min_neighbors, min_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam does not work\n",
    "<font color='red'><B>Exercise 4.</B></font>\n",
    "Instead of using the webcam, we will load an image. The function `run_detection_on_image` takes the following three parameters:\n",
    "`scale_factor`, `min_neighbors`, and `min_size` (<A HREF=\"https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html\" TARGET=\"_blank\">documentation of the OpenCV function</A>). You can change the parameters at the bottom of the following code cell.\n",
    "\n",
    "1. Can you make the algorithm run much faster by changing the parameters? Why does it become much faster? Does it influence the performance of the algorithm in any significant way?\n",
    "2. Can you make the algorithm much faster by other means, e.g., adding more parameters or intervening (adding / changing code) in the `run_detection` function? How, and why does the execution time change? Does this affect the performance in any significant way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def run_detection_on_image(image_name='./images/Guido_DelFly.jpg', scale_factor = 1.1, min_neighbors = 5, min_size = 30):\n",
    "\n",
    "    # Create the haar cascade      \n",
    "    faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    # Load the image:  \n",
    "    BGR = cv2.imread(image_name);\n",
    "    \n",
    "    # get the starting time:\n",
    "    start_time = time.time();\n",
    "    \n",
    "    # convert the image to gray scale:\n",
    "    gray = cv2.cvtColor(BGR, cv2.COLOR_BGR2GRAY);\n",
    "    \n",
    "    # detect the faces with the Viola and Jones method:\n",
    "    faces = faceCascade.detectMultiScale(gray, \n",
    "                                         scaleFactor=scale_factor, minNeighbors=min_neighbors, \n",
    "                                         minSize=(min_size, min_size));\n",
    "    \n",
    "    # get the end time:\n",
    "    end_time = time.time();\n",
    "    \n",
    "    # Report on number of faces and processing time:\n",
    "    print(\"Found {0} faces, in {1} seconds.\".format(len(faces), end_time - start_time));\n",
    "    \n",
    "    # Draw a rectangle around the found faces:\n",
    "    rectangle_color = (0, 255, 0);\n",
    "    line_width = 10;\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(BGR, (x, y), (x+w, y+h), rectangle_color, line_width)\n",
    "    \n",
    "    # Show the image:\n",
    "    plt.imshow(cv2.cvtColor(BGR, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "# play with the parameters:\n",
    "scale_factor = 1.1;\n",
    "min_neighbors = 5;\n",
    "min_size = 30;\n",
    "run_detection_on_image(scale_factor=scale_factor, min_neighbors=min_neighbors, min_size=min_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade of simple features. In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on (Vol. 1, pp. I-511). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "Exercise 2. \n",
    "\n",
    "Webcam works: The execution time can be increased by adding more texture in the scene. Then the V&J detector will not be able to reject samples as quickly as when there is less texture. More faces also means more processing time.\n",
    "\n",
    "Exercise 3 & 4.\n",
    "\n",
    "1. The parameter ``scale_factor`` influences the computation time. If set higher, less processing is performed. However, also fewer faces are found, as fewer pyramid levels are searched. The parameter ``min_neighbors`` does not influence processing time. It does influence the performance, as setting it low (e.g., 0 or 1) will result in more detections (false and true positives). The parameter ``min_size`` does influence computation time. Setting it higher means that smaller windows are not searched and reduces processing time. This also means that smaller faces are not found.\n",
    "\n",
    "2. One can add also a max size, with, e.g., ``maxSize=(60, 60)``. This will reduce processing time, and the detection of large faces. One can also reduce the image size before passing it to the V&J detector. This will mean extra processing for the scaling, but less for the detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
